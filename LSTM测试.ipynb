{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\13798\\.conda\\envs\\KAN\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "Epoch 1/998: 100%|██████████| 7248/7248 [00:23<00:00, 303.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: nan, Train Accuracy P: 0.8733, Train Accuracy M: 0.8757, Val Loss: nan, Val Accuracy: 0.6922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/998: 100%|██████████| 7248/7248 [00:24<00:00, 294.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Train Loss: nan, Train Accuracy P: 0.5695, Train Accuracy M: 0.5695, Val Loss: nan, Val Accuracy: 0.5695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/998: 100%|██████████| 7248/7248 [00:24<00:00, 294.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Train Loss: nan, Train Accuracy P: 0.5695, Train Accuracy M: 0.5695, Val Loss: nan, Val Accuracy: 0.5695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/998:  15%|█▌        | 1119/7248 [00:03<00:21, 284.61it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 205\u001b[0m\n\u001b[0;32m    203\u001b[0m loss_3 \u001b[38;5;241m=\u001b[39m criterion(output_f, y_f)\n\u001b[0;32m    204\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_1 \u001b[38;5;241m+\u001b[39m loss_2 \u001b[38;5;241m+\u001b[39m loss_3\n\u001b[1;32m--> 205\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    206\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    208\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\13798\\.conda\\envs\\KAN\\lib\\site-packages\\torch\\_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    524\u001b[0m     )\n\u001b[1;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\13798\\.conda\\envs\\KAN\\lib\\site-packages\\torch\\autograd\\__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\13798\\.conda\\envs\\KAN\\lib\\site-packages\\torch\\autograd\\graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    745\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    746\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import joblib  # 用于模型保存和加载\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "import joblib\n",
    "from sklearn.utils import shuffle\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "import datetime\n",
    "import time\n",
    "import torchvision.transforms as T\n",
    "def save_model(model:nn.Module, model_name,epoch):\n",
    "    save_folder_name = 'checkpoints'\n",
    "    if not os.path.exists(save_folder_name):\n",
    "        os.makedirs(save_folder_name)\n",
    "    # add time information btw\n",
    "    now = datetime.datetime.now()\n",
    "    timestamp = now.strftime('%Y%m%d%H%M%S')\n",
    "    model_path = os.path.join(save_folder_name, f'{model_name}_{epoch}_{timestamp}.pt')\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    \n",
    "# 设置设备\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class FatigueDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels_p, self.labels_m, self.labels_f = labels  # Separate out the three labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], (self.labels_p[idx], self.labels_m[idx], self.labels_f[idx])  # Return all three labels\n",
    "\n",
    "\n",
    "# 定义LSTM模型\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim,h_dim=512):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, h_dim, num_layers=1,batch_first=True)\n",
    "        self.bn1 = nn.BatchNorm1d(h_dim)\n",
    "        self.fc_for_p = nn.Linear(h_dim, 256)\n",
    "        self.fc_for_m = nn.Linear(h_dim, 256)\n",
    "        self.fc_for_f = nn.Linear(h_dim, 256)  # Add fully connected layer for 'F'\n",
    "        \n",
    "        self.bn2 = nn.BatchNorm1d(256)\n",
    "        self.classifier_for_p = nn.Linear(256, 4)  # 4 classes\n",
    "        self.classifier_for_m = nn.Linear(256,4)   # 4 classes\n",
    "        self.classifier_for_f = nn.Linear(256,4)   # 4 classes (for 'F')\n",
    "        self.relu = nn.ReLU()\n",
    "        self.h_dim = h_dim\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(1, x.size(0), self.h_dim).to(device)\n",
    "        c0 = torch.zeros(1, x.size(0), self.h_dim).to(device)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        \n",
    "        out = self.bn1(out[:, -1, :])  # Use the last hidden state\n",
    "        \n",
    "        out_p = self.relu(self.fc_for_p(out))\n",
    "        # out_m = self.relu(self.fc_for_m(out))\n",
    "        # out_f = self.relu(self.fc_for_f(out))  # Add this for 'F'\n",
    "        \n",
    "        out_p = self.bn2(out_p)\n",
    "        # out_m = self.bn2(out_m)\n",
    "        # out_f = self.bn2(out_f)  # Add BatchNorm for 'F'\n",
    "        \n",
    "        out_p = self.classifier_for_p(out_p)\n",
    "        # out_m = self.classifier_for_m(out_m)\n",
    "        # out_f = self.classifier_for_f(out_f)  # Add classifier for 'F'\n",
    "        \n",
    "        return out_p#, out_m, out_f  # Return all three outputs\n",
    "\n",
    "\n",
    "# 定义早停机制\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=998, delta=0):\n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "    \n",
    "    def __call__(self, val_loss, model):\n",
    "        score = -val_loss\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "    \n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        torch.save(model.state_dict(), 'LSTM_model.pt')\n",
    "        self.val_loss_min = val_loss\n",
    "\n",
    "# 加载数据\n",
    "train_df1 = pd.read_csv('FAA_train.csv')\n",
    "#train_df2 = pd.read_csv('CNNMEFAR60_train+.csv')\n",
    "train_df = pd.concat([train_df1])\n",
    "\n",
    "test_df1 = pd.read_csv('FAA_test.csv')\n",
    "#test_df2 = pd.read_csv('CNNMEFAR40_test+.csv')\n",
    "test_df = pd.concat([test_df1])\n",
    "\n",
    "val_df1 = pd.read_csv('FAA_val.csv')\n",
    "#val_df2 = pd.read_csv('CNNMEFAR60_val+.csv')\n",
    "val_df = pd.concat([val_df1])\n",
    "\n",
    "\n",
    "# 特征选择\n",
    "features = ['bvp','acc_x','acc_y','acc_z','eda','hr','temperature']\n",
    "\n",
    "X_train = train_df[features]\n",
    "y_train_p = train_df['Numeric Workload']\n",
    "#y_train_m = train_df['Numeric Workload']\n",
    "#y_train_f = train_df['Numeric Workload']\n",
    "\n",
    "X_val = val_df[features]\n",
    "y_val_p = val_df['Numeric Workload']\n",
    "# y_val_m = val_df['Numeric Workload']\n",
    "# y_val_f = val_df['Numeric Workload']\n",
    "\n",
    "X_test = test_df[features]\n",
    "y_test_p = test_df['Numeric Workload']\n",
    "# y_test_m = test_df['Numeric Workload']\n",
    "# y_test_f = test_df['Numeric Workload']\n",
    "# 数据标准化\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 转换为PyTorch张量\n",
    "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32).to(device)\n",
    "X_val_tensor = torch.tensor(X_val_scaled, dtype=torch.float32).to(device)\n",
    "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32).to(device)\n",
    "y_train_tensor_p = torch.tensor(y_train_p.values, dtype=torch.long).to(device)  # Change to long for CE loss\n",
    "y_val_tensor_p = torch.tensor(y_val_p.values, dtype=torch.long).to(device)  # Change to long for CE loss\n",
    "y_test_tensor_p = torch.tensor(y_test_p.values, dtype=torch.long).to(device)  # Change to long for CE loss\n",
    "# y_train_tensor_m = torch.tensor(y_train_m.values, dtype=torch.long).to(device)  # Change to long for CE loss\n",
    "# y_val_tensor_m = torch.tensor(y_val_m.values, dtype=torch.long).to(device)  # Change to long for CE loss\n",
    "# y_test_tensor_m = torch.tensor(y_test_m.values, dtype=torch.long).to(device)  # Change to long for CE loss\n",
    "# y_train_tensor_f = torch.tensor(y_train_f.values, dtype=torch.long).to(device)  # Change to long for CE loss\n",
    "# y_val_tensor_f = torch.tensor(y_val_f.values, dtype=torch.long).to(device)  # Change to long for CE loss\n",
    "# y_test_tensor_f = torch.tensor(y_test_f.values, dtype=torch.long).to(device)  # Change to long for CE loss\n",
    "\n",
    "# 对数据进行LSTM的reshape\n",
    "X_train_tensor = X_train_tensor.unsqueeze(1)  # 变成 (batch_size, seq_len, input_dim)\n",
    "X_val_tensor = X_val_tensor.unsqueeze(1)\n",
    "X_test_tensor = X_test_tensor.unsqueeze(1)\n",
    "\n",
    "# 创建数据加载器\n",
    "# train_dataset = FatigueDataset(X_train_tensor, (y_train_tensor_p, y_train_tensor_m, y_train_tensor_f))\n",
    "# val_dataset = FatigueDataset(X_val_tensor, (y_val_tensor_p, y_val_tensor_m, y_val_tensor_f))\n",
    "# test_dataset = FatigueDataset(X_test_tensor, (y_test_tensor_p, y_test_tensor_m, y_test_tensor_f))\n",
    "\n",
    "train_dataset = FatigueDataset(X_train_tensor, y_train_tensor_p)\n",
    "val_dataset = FatigueDataset(X_val_tensor, y_val_tensor_p)\n",
    "test_dataset = FatigueDataset(X_test_tensor, y_test_tensor_p)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# 初始化模型、损失函数和优化器\n",
    "model = LSTMModel(input_dim=X_train.shape[1]).to(device)\n",
    "criterion = nn.CrossEntropyLoss()  # Change to CrossEntropyLoss\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, verbose=True)\n",
    "early_stopping = EarlyStopping(patience=5, delta=0.001)\n",
    "\n",
    "# 训练模型\n",
    "num_epochs = 998\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    correct_p = 0\n",
    "    correct_m = 0\n",
    "    correct_f = 0\n",
    "    total_p = 0\n",
    "    total_m = 0\n",
    "    total_f = 0\n",
    "    with tqdm(total=len(train_loader), desc=f\"Epoch {epoch + 1}/{num_epochs}\") as pbar:\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            y_p, y_m ,y_f= y_batch\n",
    "            optimizer.zero_grad()\n",
    "            output_p, output_m , output_f= model(X_batch)\n",
    "            loss_1 = criterion(output_p, y_p)\n",
    "            loss_2 = criterion(output_m, y_m)\n",
    "            loss_3 = criterion(output_f, y_f)\n",
    "            loss = loss_1 + loss_2 + loss_3\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            _, predicted_p = torch.max(output_p.data, 1)\n",
    "            _, predicted_m = torch.max(output_m.data, 1)\n",
    "            _, predicted_f = torch.max(output_f.data, 1)\n",
    "            total_p += y_p.size(0)\n",
    "            total_m += y_m.size(0)\n",
    "            total_f += y_f.size(0)\n",
    "            correct_p += (predicted_p == y_p).sum().item()\n",
    "            correct_m += (predicted_m == y_m).sum().item()\n",
    "            correct_f += (predicted_f == y_f).sum().item()\n",
    "            \n",
    "            pbar.update(1)\n",
    "    \n",
    "    train_accuracy_p = correct_p / total_p\n",
    "    train_accuracy_m = correct_m / total_m\n",
    "    train_accuracy_f = correct_f / total_f\n",
    "    if epoch % 5 == 0:\n",
    "        save_model(model, 'LSTM', epoch)\n",
    "    scheduler.step(train_loss)\n",
    "    \n",
    "    # 验证模型\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct_p = 0\n",
    "    correct_m = 0\n",
    "    total_p = 0\n",
    "    total_m = 0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            y_p, y_m,y_f = y_batch\n",
    "            output_p, output_m,output_f = model(X_batch)\n",
    "            loss_1 = criterion(output_p, y_p)\n",
    "            loss_2 = criterion(output_m, y_m)\n",
    "            loss_3 = criterion(output_f, y_f)\n",
    "            loss = loss_1 + loss_2+loss_3\n",
    "    \n",
    "            \n",
    "            val_loss += loss.item()\n",
    "            _, predicted_p = torch.max(output_p.data, 1)\n",
    "            _, predicted_m = torch.max(output_m.data, 1)\n",
    "            _, predicted_f = torch.max(output_f.data, 1)\n",
    "            total_p += y_p.size(0)\n",
    "            total_m += y_m.size(0)\n",
    "            total_f += y_f.size(0)\n",
    "            correct_p += (predicted_p == y_p).sum().item()\n",
    "            correct_m += (predicted_m == y_m).sum().item()\n",
    "            correct_f += (predicted_f == y_f).sum().item()\n",
    "    \n",
    "    val_accuracy_p = correct_p / total_p\n",
    "    val_accuracy_m = correct_m / total_m\n",
    "    val_accuracy_f = correct_f / total_f\n",
    "    val_accuracy = (correct_p + correct_m+correct_f) / (total_p + total_m+total_f)\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}, Train Loss: {train_loss:.4f}, Train Accuracy P: {train_accuracy_p:.4f}, Train Accuracy M: {train_accuracy_m:.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}\")\n",
    "    \n",
    "    early_stopping(val_loss, model)\n",
    "    \n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping\")\n",
    "        break\n",
    "\n",
    "# 加载最佳模型\n",
    "model.load_state_dict(torch.load('LSTM_model.pt'))\n",
    "joblib.dump(scaler, 'LSTMscaler.pkl')\n",
    "\n",
    "# 预测\n",
    "model.eval()\n",
    "y_pred_proba_p = []\n",
    "y_pred_proba_m = []\n",
    "y_pred_proba_f = []\n",
    "y_true_p = []\n",
    "y_true_m = []\n",
    "y_true_f = []\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        y_p, y_m,y_f = y_batch\n",
    "        output_p, output_m,output_f = model(X_batch)\n",
    "        y_pred_proba_p.extend(output_p.cpu().numpy())\n",
    "        y_pred_proba_m.extend(output_m.cpu().numpy())\n",
    "        y_pred_proba_f.extend(output_f.cpu().numpy())\n",
    "        y_true_p.extend(y_p.cpu().numpy())\n",
    "        y_true_m.extend(y_m.cpu().numpy())\n",
    "        y_true_f.extend(y_f.cpu().numpy())\n",
    "\n",
    "y_pred_p = np.argmax(y_pred_proba_p, axis=1)\n",
    "y_pred_m = np.argmax(y_pred_proba_m, axis=1)\n",
    "y_pred_f = np.argmax(y_pred_proba_f, axis=1)\n",
    "\n",
    "# 评估模型\n",
    "accuracy_p = accuracy_score(y_true_p, y_pred_p)\n",
    "accuracy_m = accuracy_score(y_true_m, y_pred_m)\n",
    "accuracy_f = accuracy_score(y_true_f, y_pred_f)\n",
    "print(f\"LSTM Model Accuracy P: {accuracy_p:.4f}\")\n",
    "print(f\"LSTM Model Accuracy M: {accuracy_m:.4f}\")\n",
    "print(f\"LSTM Model Accuracy F: {accuracy_f:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\13798\\.conda\\envs\\YOLO\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "Epoch 1/998:  71%|███████▏  | 5176/7248 [00:17<00:07, 270.59it/s]"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "def save_model(model: nn.Module, model_name, epoch):\n",
    "    save_folder_name = 'checkpoints'\n",
    "    if not os.path.exists(save_folder_name):\n",
    "        os.makedirs(save_folder_name)\n",
    "    now = datetime.datetime.now()\n",
    "    timestamp = now.strftime('%Y%m%d%H%M%S')\n",
    "    model_path = os.path.join(save_folder_name, f'{model_name}_{epoch}_{timestamp}.pt')\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "\n",
    "# 设置设备\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class FatigueDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n",
    "\n",
    "# 定义LSTM模型\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, h_dim=512):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, h_dim, num_layers=1, batch_first=True)\n",
    "        self.bn1 = nn.BatchNorm1d(h_dim)\n",
    "        self.fc = nn.Linear(h_dim, 256)\n",
    "        self.bn2 = nn.BatchNorm1d(256)\n",
    "        self.classifier = nn.Linear(256, 4)  # 4 classes\n",
    "        self.relu = nn.ReLU()\n",
    "        self.h_dim = h_dim\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(1, x.size(0), self.h_dim).to(device)\n",
    "        c0 = torch.zeros(1, x.size(0), self.h_dim).to(device)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        \n",
    "        out = self.bn1(out[:, -1, :])  # Use the last hidden state\n",
    "        out = self.relu(self.fc(out))\n",
    "        out = self.bn2(out)\n",
    "        out = self.classifier(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "# 定义早停机制\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, delta=0):\n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "    \n",
    "    def __call__(self, val_loss, model):\n",
    "        score = -val_loss\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "    \n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        torch.save(model.state_dict(), 'LSTM_model.pt')\n",
    "        self.val_loss_min = val_loss\n",
    "\n",
    "# 加载数据\n",
    "train_df = pd.read_csv('FAA_train.csv')\n",
    "test_df = pd.read_csv('FAA_test.csv')\n",
    "val_df = pd.read_csv('FAA_val.csv')\n",
    "\n",
    "# 特征选择\n",
    "features = ['bvp', 'acc_x', 'acc_y', 'acc_z', 'eda', 'hr', 'temperature']\n",
    "\n",
    "X_train = train_df[features]\n",
    "y_train = train_df['Numeric Workload']\n",
    "X_val = val_df[features]\n",
    "y_val = val_df['Numeric Workload']\n",
    "X_test = test_df[features]\n",
    "y_test = test_df['Numeric Workload']\n",
    "\n",
    "# 数据标准化\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 转换为PyTorch张量\n",
    "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32).to(device)\n",
    "X_val_tensor = torch.tensor(X_val_scaled, dtype=torch.float32).to(device)\n",
    "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32).to(device)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.long).to(device)\n",
    "y_val_tensor = torch.tensor(y_val.values, dtype=torch.long).to(device)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.long).to(device)\n",
    "\n",
    "# 对数据进行LSTM的reshape\n",
    "X_train_tensor = X_train_tensor.unsqueeze(1)  # 变成 (batch_size, seq_len, input_dim)\n",
    "X_val_tensor = X_val_tensor.unsqueeze(1)\n",
    "X_test_tensor = X_test_tensor.unsqueeze(1)\n",
    "\n",
    "# 创建数据加载器\n",
    "train_dataset = FatigueDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = FatigueDataset(X_val_tensor, y_val_tensor)\n",
    "test_dataset = FatigueDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# 初始化模型、损失函数和优化器\n",
    "model = LSTMModel(input_dim=X_train.shape[1]).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, verbose=True)\n",
    "early_stopping = EarlyStopping(patience=5, delta=0.001)\n",
    "\n",
    "# 训练模型\n",
    "num_epochs = 998\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with tqdm(total=len(train_loader), desc=f\"Epoch {epoch + 1}/{num_epochs}\") as pbar:\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(X_batch)\n",
    "            loss = criterion(output, y_batch)\n",
    "            if loss.item() == float('inf'):\n",
    "                print(X_batch)\n",
    "                print(y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            total += y_batch.size(0)\n",
    "            correct += (predicted == y_batch).sum().item()\n",
    "            \n",
    "            pbar.update(1)\n",
    "    \n",
    "    train_accuracy = correct / total\n",
    "    if epoch % 5 == 0:\n",
    "        save_model(model, 'LSTM', epoch)\n",
    "    scheduler.step(train_loss)\n",
    "    \n",
    "    # 验证模型\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            output = model(X_batch)\n",
    "            loss = criterion(output, y_batch)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            total += y_batch.size(0)\n",
    "            correct += (predicted == y_batch).sum().item()\n",
    "    \n",
    "    val_accuracy = correct / total\n",
    "    print(f\"Epoch {epoch + 1}, Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}\")\n",
    "    \n",
    "    early_stopping(val_loss, model)\n",
    "    \n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping\")\n",
    "        break\n",
    "\n",
    "# 加载最佳模型\n",
    "model.load_state_dict(torch.load('LSTM_model.pt'))\n",
    "joblib.dump(scaler, 'LSTMscaler.pkl')\n",
    "\n",
    "# 预测\n",
    "model.eval()\n",
    "y_pred_proba = []\n",
    "y_true = []\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        output = model(X_batch)\n",
    "        y_pred_proba.extend(output.cpu().numpy())\n",
    "        y_true.extend(y_batch.cpu().numpy())\n",
    "\n",
    "y_pred = np.argmax(y_pred_proba, axis=1)\n",
    "\n",
    "# 评估模型\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "print(f\"LSTM Model Accuracy: {accuracy:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "KAN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
